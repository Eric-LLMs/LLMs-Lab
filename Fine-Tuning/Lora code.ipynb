{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd5a92c5bd8276b",
   "metadata": {},
   "source": [
    "Source code : \n",
    "https://huggingface.co/transformers/v4.3.3/_modules/transformers/models/roberta/modeling_roberta.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc43862",
   "metadata": {},
   "source": [
    "![](images/LoRA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraRobertaSelfAttention(RobertaSelfAttention):\n",
    "    \"\"\"\n",
    "    Inherits from the RobertaSelfAttention module.\n",
    "    Creates low-rank matrices for the query and value.\n",
    "\n",
    "    Parameters:\n",
    "    - r (int): Rank for LoRA matrices.\n",
    "    - config: Configuration of the Roberta Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, r=8, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        d = self.all_head_size  # all_head_size = dimension * number_attention_heads\n",
    "\n",
    "        # Initialize LoRA matrices for query and value\n",
    "        self.lora_query_matrix_B = nn.Parameter(torch.zeros(d, r))\n",
    "        self.lora_query_matrix_A = nn.Parameter(torch.randn(r, d))\n",
    "        self.lora_value_matrix_B = nn.Parameter(torch.zeros(d, r))\n",
    "        self.lora_value_matrix_A = nn.Parameter(torch.randn(r, d))\n",
    "    \n",
    "    def lora_query(self, x):\n",
    "        \"\"\"\n",
    "        Obtain the query matrix: original matrix + LoRA matrix. \n",
    "        The original matrix does not need to be updated.\n",
    "        Wx --> Wx + BAx\n",
    "        \"\"\"\n",
    "        lora_query_weights = torch.matmul(self.lora_query_matrix_B, self.lora_query_matrix_A)\n",
    "        return self.query(x) + F.linear(x, lora_query_weights)\n",
    "    \n",
    "    def lora_value(self, x):\n",
    "        \"\"\"\n",
    "        Obtain the value matrix: original matrix + LoRA matrix. \n",
    "        The original matrix does not need to be updated.\n",
    "        \"\"\"\n",
    "        lora_value_weights = torch.matmul(self.lora_value_matrix_B, self.lora_value_matrix_A)\n",
    "        return self.value(x) + F.linear(x, lora_value_weights)\n",
    "    \n",
    "    def forward(self, hidden_states, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Only the self.query() and self.value() functions are affected, and need to be replaced.\n",
    "        \"\"\"\n",
    "        # Original code for query:\n",
    "        ## mixed_query_layer = self.query(hidden_states)\n",
    "        # Updated query for LoRA:\n",
    "        mixed_query_layer = self.lora_query(hidden_states)\n",
    "\n",
    "        # The key has no LoRA, so leave these calls unchanged\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "\n",
    "        # Original code for value:\n",
    "        ## value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        # Updated value for LoRA:\n",
    "        value_layer = self.transpose_for_scores(self.lora_value(hidden_states))\n",
    "        \n",
    "        # Other operations...\n",
    "\n",
    "\n",
    "class LoraWrapperRoberta(nn.Module):\n",
    "    def __init__(self, task_type, num_classes=None, dropout_rate=0.1, model_id=\"roberta-large\",\n",
    "                 lora_rank=8, train_biases=True, train_embedding=False, train_layer_norms=True):\n",
    "        \"\"\"\n",
    "        RoBERTa Wrapper, requires replacing all self-attention layers in the original model.\n",
    "        - task_type: Type of NLP task ('glue', 'squad_v1', 'squad_v2').\n",
    "        - num_classes: Number of classes for classification (varies with task).\n",
    "        - dropout_rate: Dropout rate in the model.\n",
    "        - model_id: Pre-trained RoBERTa model ID.\n",
    "        - lora_rank: Rank for LoRA adaptation.\n",
    "        - train_biases, train_embedding, train_layer_norms: \n",
    "            Flags whether to keep certain parameters trainable \n",
    "            after initializing LoRA.\n",
    "        \n",
    "        Example:\n",
    "            model = LoraWrapperRoberta(task_type='glue')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 1. Initialize the base model with parameters\n",
    "        self.model_id = model_id\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_id)\n",
    "        self.model = RobertaModel.from_pretrained(model_id)\n",
    "        self.model_config = self.model.config\n",
    "\n",
    "        # 2. Add the layer for the benchmark tasks\n",
    "        d_model = self.model_config.hidden_size\n",
    "        self.finetune_head_norm = nn.LayerNorm(d_model)\n",
    "        self.finetune_head_dropout = nn.Dropout(dropout_rate)\n",
    "        self.finetune_head_classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "        # 3. Set up the LoRA model for training\n",
    "        self.replace_multihead_attention()\n",
    "        self.freeze_parameters_except_lora_and_bias()\n",
    "        \n",
    "    def replace_multihead_attention_recursion(self, model):\n",
    "        \"\"\"\n",
    "        Replace RobertaSelfAttention in the model with LoraRobertaSelfAttention.\n",
    "        This method applies the replacement recursively to all sub-components.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            The PyTorch module or model to be modified.\n",
    "        \"\"\"\n",
    "        for name, module in model.named_children():\n",
    "            if isinstance(module, RobertaSelfAttention):\n",
    "                # Replace RobertaSelfAttention with LoraRobertaSelfAttention\n",
    "                new_layer = LoraRobertaSelfAttention(r=self.lora_rank, config=self.model_config)\n",
    "                new_layer.load_state_dict(module.state_dict(), strict=False)\n",
    "                setattr(model, name, new_layer)\n",
    "            else:\n",
    "                # Recursive call for child modules\n",
    "                self.replace_multihead_attention_recursion(module)\n",
    "                \n",
    "    def freeze_parameters_except_lora_and_bias(self):\n",
    "        \"\"\"\n",
    "        Freeze some parameters based on the predefined configuration, \n",
    "        so they do not need to be trained during the fine-tuning phase.\n",
    "        \n",
    "        The parameters in the LoRA layers, the fine-tune head, bias parameters, \n",
    "        embeddings, and layer norms can be set as trainable parameters.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            is_trainable = (\n",
    "                \"lora_\" in name or\n",
    "                \"finetune_head_\" in name or\n",
    "                (self.train_biases and \"bias\" in name) or\n",
    "                (self.train_embeddings and \"embeddings\" in name) or\n",
    "                (self.train_layer_norms and \"LayerNorm\" in name)\n",
    "            )\n",
    "            param.requires_grad = is_trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1267b62dfc9baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoraLinear is the smallest modifiable unit\n",
    "class LoraLinear(nn.Linear):  # wx --> wx + BAx\n",
    "    \"\"\"\n",
    "    Extends a PyTorch linear layer with Low-Rank Adaptation (LoRA).\n",
    "    LoRA adds two matrices to the layer, allowing for efficient training of large models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, r=8, *args, **kwargs):\n",
    "        super().__init__(in_features, out_features, *args, **kwargs)\n",
    "\n",
    "        # Initialize LoRA matrices\n",
    "        self.lora_matrix_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        self.lora_matrix_A = nn.Parameter(torch.randn(r, in_features))\n",
    "\n",
    "        # Freeze the original weight matrix\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Compute LoRA weight adjustment\n",
    "        lora_weights = torch.matmul(self.lora_matrix_B, self.lora_matrix_A)\n",
    "        # Apply the original and LoRA-adjusted linear transformations\n",
    "        return super().forward(x) + F.linear(x, lora_weights)\n",
    "\n",
    "\n",
    "# LoRA fine-tuning on flan-T5-xxl\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", load_in_8bit=True, device_map=\"auto\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,  # \\hat{W} = W + \\alpha/r * \\delta{W}\n",
    "    target_modules=[\"q\", \"v\"],  # Which matrices to learn in Q, K, V?\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",  # Do not train the bias term\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# The rest of the training process is the same as before!\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
