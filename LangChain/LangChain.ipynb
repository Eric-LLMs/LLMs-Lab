{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8d8b9e",
   "metadata": {},
   "source": [
    "### Core Components of LangChain\n",
    "\n",
    "1. **Model I/O Wrapping**\n",
    "   - **LLMs**: Large Language Models\n",
    "   - **Chat Models**: Generally based on LLMs but restructured for conversational purposes\n",
    "   - **PromptTemplate**: Templates for prompt creation\n",
    "   - **OutputParser**: Parses the output from models\n",
    "     \n",
    "2. **Data Connection Wrapping**\n",
    "   - **Document Loaders**: Loaders for various file formats\n",
    "   - **Document Transformers**: Common operations on documents such as splitting, filtering, translating, and extracting metadata\n",
    "   - **Text Embedding Models**: Convert text into vector representations, useful for tasks like retrieval\n",
    "   - **Vectorstores**: Stores for vectors (used in retrieval tasks)\n",
    "   - **Retrievers**: Tools for retrieving vectors from storage\n",
    "\n",
    "3. **Memory Wrapping**\n",
    "   - **Memory**: Not physical memory; it manages \"context\", \"history\", or \"memory\" from a text perspective\n",
    "\n",
    "4. **Architecture Wrapping**\n",
    "   - **Chain**: Implements a single function or a series of sequential functions\n",
    "   - **Agent**: Automatically plans and executes steps based on user input, selecting the necessary tools for each step to achieve the desired task\n",
    "     - **Tools**: Functions for calling external functionalities, such as Google search, file I/O, Linux shell, etc.\n",
    "     - **Toolkits**: A set of tools designed to operate specific software, such as a toolkit for managing databases or Gmail\n",
    "\n",
    "5. **Callbacks**\n",
    "\n",
    "![LangChain Components](data/langchain.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5688f7b7",
   "metadata": {},
   "source": [
    "### Documentation (Taking the Python version as an example)\n",
    "- Functional Modules: https://python.langchain.com/docs/get_started/introduction\n",
    "- API Documentation: https://api.python.langchain.com/en/latest/langchain_api_reference.html\n",
    "- Third-Party Component Integration: https://python.langchain.com/docs/integrations/platforms/\n",
    "- Official Use Cases: https://python.langchain.com/docs/use_cases\n",
    "- Debugging, Deployment, and Other Guidance: https://python.langchain.com/docs/guides/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b5b9e",
   "metadata": {},
   "source": [
    "## 1. Model I/O Encapsulation\n",
    "Different models are encapsulated into a unified interface, making it easier to switch models without restructuring the code.\n",
    "\n",
    "### 1.1 Model API: LLM vs. ChatModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f796d0d24a1e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1173de87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T17:05:52.828601500Z",
     "start_time": "2024-09-22T17:05:52.794799300Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langchain==0.1.20\n",
    "# !pip install --upgrade langchain-openai==0.1.6\n",
    "# !pip install langchain-community==0.038\n",
    "# pip install --upgrade langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f626ae21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T17:05:55.187345200Z",
     "start_time": "2024-09-22T17:05:55.167266600Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"\"\n",
    "\n",
    "# OPENAI_API_KEY = \"sk-xxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a21c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T17:05:56.588459300Z",
     "start_time": "2024-09-22T17:05:56.576402200Z"
    }
   },
   "source": [
    "### 1.1.1 OpenAI Model Wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41f4c092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T17:05:58.354434300Z",
     "start_time": "2024-09-22T17:05:57.771953500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a language model AI created by OpenAI. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # default gpt-3.5-turbo\n",
    "response = llm.invoke(\"who are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928a9b6",
   "metadata": {},
   "source": [
    "### 1.1.2 Multi-turn Conversation Session Wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "637e39e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are Eric, a space ranger dedicated to exploring and protecting the galaxy.' response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 59, 'total_tokens': 74, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5a52b595-75b3-47ff-9bba-b4fc6a452066-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,  # Equivalent to the assistant role in the OpenAI API\n",
    "    HumanMessage,  # Equivalent to the user role in the OpenAI API\n",
    "    SystemMessage  # Equivalent to the system role in the OpenAI API\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are the intelligent regulatory system of the future.\"),\n",
    "    HumanMessage(content=\"I am a space ranger, my name is Eric.\"),\n",
    "    AIMessage(content=\"I am an NPC, the Earth caretaker, welcome back to Earth!\"),\n",
    "    HumanMessage(content=\"Who am I?\")\n",
    "]\n",
    "\n",
    "ret = llm.invoke(messages)\n",
    "\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73032f68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> <b>Key Point:</b> Achieve a unified interface for different models through encapsulation. </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691dbda6",
   "metadata": {},
   "source": [
    "### 1.1.3 Switching to Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0784c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ebd445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a language model AI created by OpenAI. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Other models are encapsulated in the langchain_community package.\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "\n",
    "llm = QianfanChatEndpoint(\n",
    "    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),\n",
    "    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Who are you?\")\n",
    "]\n",
    "\n",
    "ret = llm.invoke(messages)\n",
    "\n",
    "print(ret.content)\n",
    "\n",
    "# from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "# from langchain_core.messages import HumanMessage\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# import os\n",
    "\n",
    "# llm = ChatTongyi(model_name=\"qwen-vl-chat-v1\")\n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(\n",
    "#         content=\"You are a helpful assistant. When people ask questions, always start with a greeting, 'Hello!', then answer the question.\"\n",
    "#     ),\n",
    "#     HumanMessage(\n",
    "#         content=\"Hello, please introduce yourself.\"\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# ret = llm.invoke(messages)\n",
    "\n",
    "# print(ret.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afdad79",
   "metadata": {},
   "source": [
    "### 1.2 Model Input and Output\n",
    "<img src=\"data/model_io.jpg\" style=\"margin-left: 0px\" width=500px>  \n",
    "\n",
    "### 1.2.1 Prompt Template Encapsulation\n",
    "1. The PromptTemplate allows for custom variables within the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d73528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['subject'] template='Tell me a joke about {subject}.'\n",
      "===Prompt===\n",
      "Tell me a joke about ChatGPT.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"Tell me a joke about {subject}.\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(subject='ChatGPT'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e06bbb",
   "metadata": {},
   "source": [
    "2. ChatPromptTemplate: Representing Conversation Context with a Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e2d85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Baymax, the customer service assistant for Crazy Zoo. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "           \"You are the customer service assistant for {product}. Your name is {name}.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = template.format_messages(\n",
    "    product=\"Crazy Zoo\",\n",
    "    name=\"Baymax\",\n",
    "    query=\"Who are you?\"\n",
    ")\n",
    "\n",
    "ret = llm.invoke(prompt)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610342c5",
   "metadata": {},
   "source": [
    "3. MessagesPlaceholder: Turning Multi-turn Conversations into Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3b1aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Translate your answer to {language}.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    # variable_name is the message placeholder variable name in the template\n",
    "    # used for assigning values.\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e7d51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Who is Elon Musk?'), AIMessage(content='Elon Musk is a billionaire entrepreneur, inventor, and industrial designer'), HumanMessage(content='Translate your answer to 中文.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"Who is Elon Musk?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"Elon Musk is a billionaire entrepreneur, inventor, and industrial designer\"\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_prompt(\n",
    "    # Used to assign values to \"conversation\" and \"language.\"\n",
    "    conversation=[human_message, ai_message], language=\"中文\"\n",
    ")\n",
    "\n",
    "print(messages.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "887529f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "埃隆·马斯克是一位亿万富翁企业家、发明家和工业设计师。\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d398eaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> <b>Key Point:</b> View the Prompt template as a function with parameters, analogous to SK's Semantic Function. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf53c16",
   "metadata": {},
   "source": [
    "### 1.2.2 Loading Prompt Templates from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b09d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['topic'] template='举一个关于{topic}的例子'\n",
      "===Prompt===\n",
      "举一个关于Dark Humor的例子\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_file(\"data/example_prompt_template.txt\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(topic='Dark Humor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fbc57",
   "metadata": {},
   "source": [
    "### 1.3 Output Encapsulation: OutputParser\n",
    "Automatically loads the strings output by the LLM in the specified format.\n",
    "\n",
    "Built-in OutputParsers in LangChain include:\n",
    "\n",
    "- ListParser\n",
    "- DatetimeParser\n",
    "- EnumParser\n",
    "- JsonOutputParser\n",
    "- PydanticParser\n",
    "- XMLParser\n",
    "And more.\n",
    "\n",
    "### 1.3.1 Pydantic (JSON) Parser\n",
    "Automatically generates output format specifications based on the definitions in Pydantic classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f16029b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:33:22.354025800Z",
     "start_time": "2024-09-22T18:33:22.320279100Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List, Dict\n",
    "\n",
    "# Define your output object\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")\n",
    "\n",
    "    # ----- Optional Mechanism --------\n",
    "    # You can add custom validation mechanisms\n",
    "    @validator('month')\n",
    "    def valid_month(cls, field):\n",
    "        if field <= 0 or field > 12:\n",
    "            raise ValueError(\"Month must be between 1 and 12\")\n",
    "        return field\n",
    "\n",
    "    @validator('day')\n",
    "    def valid_day(cls, field):\n",
    "        if field <= 0 or field > 31:\n",
    "            raise ValueError(\"Day must be between 1 and 31\")\n",
    "        return field\n",
    "\n",
    "    @validator('day', pre=True, always=True)\n",
    "    def valid_date(cls, day, values):\n",
    "        year = values.get('year')\n",
    "        month = values.get('month')\n",
    "\n",
    "        # Ensure both year and month are provided\n",
    "        if year is None or month is None:\n",
    "            return day  # Cannot validate the date without year and month\n",
    "\n",
    "        # Check if the date is valid\n",
    "        if month == 2:\n",
    "            if cls.is_leap_year(year) and day > 29:\n",
    "                raise ValueError(\"February has a maximum of 29 days in a leap year\")\n",
    "            elif not cls.is_leap_year(year) and day > 28:\n",
    "                raise ValueError(\"February has a maximum of 28 days in a non-leap year\")\n",
    "        elif month in [4, 6, 9, 11] and day > 30:\n",
    "            raise ValueError(f\"{month} has a maximum of 30 days\")\n",
    "\n",
    "        return day\n",
    "\n",
    "    @staticmethod\n",
    "    def is_leap_year(year):\n",
    "        if year % 400 == 0 or (year % 4 == 0 and year % 100 != 0):\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "213e0f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:33:24.836446900Z",
     "start_time": "2024-09-22T18:33:23.204465500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Format Instruction=====\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"title\": \"Year\", \"description\": \"Year\", \"type\": \"integer\"}, \"month\": {\"title\": \"Month\", \"description\": \"Month\", \"type\": \"integer\"}, \"day\": {\"title\": \"Day\", \"description\": \"Day\", \"type\": \"integer\"}, \"era\": {\"title\": \"Era\", \"description\": \"BC or AD\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "====Prompt=====\n",
      "Extract the date from user input.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"title\": \"Year\", \"description\": \"Year\", \"type\": \"integer\"}, \"month\": {\"title\": \"Month\", \"description\": \"Month\", \"type\": \"integer\"}, \"day\": {\"title\": \"Day\", \"description\": \"Day\", \"type\": \"integer\"}, \"era\": {\"title\": \"Era\", \"description\": \"BC or AD\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "User input:\n",
      "The weather was clear on April 6, 2023...\n",
      "====Raw Output from Model=====\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 4,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "====Parsed Output=====\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "temperature = 0\n",
    "model = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# Construct an OutputParser based on the Pydantic object definition\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "template = \"\"\"Extract the date from user input.\n",
    "{format_instructions}\n",
    "User input:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\"],\n",
    "    # Directly retrieve output description from the OutputParser and pre-assign values to the template variables\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "print(\"====Format Instruction=====\")\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "query = \"The weather was clear on April 6, 2023...\"\n",
    "model_input = prompt.format_prompt(query=query)\n",
    "\n",
    "print(\"====Prompt=====\")\n",
    "print(model_input.to_string())\n",
    "\n",
    "output = model.invoke(model_input.to_messages())\n",
    "print(\"====Raw Output from Model=====\")\n",
    "print(output.content)\n",
    "print(\"====Parsed Output=====\")\n",
    "date = parser.parse(output.content)\n",
    "print(date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d6e46",
   "metadata": {},
   "source": [
    "### 1.3.2 Auto-Fixing Parser\n",
    "\n",
    "Automatically repairs and re-parses based on parsing exceptions using the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ba613fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Formatted Error Output===\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 四月,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===An Exception Occurred===\n",
      "Invalid json output: {\n",
      "  \"year\": 2023,\n",
      "  \"month\": 四月,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===Re-parsed Result===\n",
      "{\"year\": 2023, \"month\": 4, \"day\": 6, \"era\": \"AD\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(\n",
    "    parser=parser, llm=ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "# We correct the format of the previous output\n",
    "output = output.content.replace(\"4\", \"四月\")\n",
    "print(\"===Formatted Error Output===\")\n",
    "print(output)\n",
    "try:\n",
    "    date = parser.parse(output)\n",
    "except Exception as e:\n",
    "    print(\"===An Exception Occurred===\")\n",
    "    print(e)\n",
    "\n",
    "# Automatically fix and parse using OutputFixingParser\n",
    "date = new_parser.parse(output)\n",
    "print(\"===Re-parsed Result===\")\n",
    "print(date.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd095e97",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> <b>Thought:</b> Guess how OutputFixingParser achieves this. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4bab13",
   "metadata": {},
   "source": [
    "### 1.4 Summary\n",
    "1. LangChain provides a unified interface for calling various models, including both completion and conversational models.\n",
    "2. LangChain offers the PromptTemplate class, allowing custom templates with variables.\n",
    "3. LangChain provides a series of output parsers for converting the outputs of large models into structured objects, with built-in auto-fix functionality.\n",
    "4. The aforementioned models are some of the more outstanding parts of LangChain; however, the maintenance of the OutputParser's prompts is coupled with the code, which can be a drawback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783f7dd",
   "metadata": {},
   "source": [
    "## 2. Data Connection Encapsulation\n",
    "<img src=\"data/data_connection.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 2.1 Document Loaders: Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4f1af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a852881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗Louis Martin†Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229766e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"> The implementations of PDFLoader and TextSplitter in LangChain are relatively rough and are not recommended for use in production. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc36c71",
   "metadata": {},
   "source": [
    "### 2.3 Vector Databases and Vector Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93c33b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!pip install chromadb\n",
    "# __import__('pysqlite3')\n",
    "# import sys\n",
    "\n",
    "# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d041c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\sk_env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\n",
      "but are not releasing.§\n",
      "2.Llama 2-Chat , a ﬁne-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "------\n",
      "Sergey Edunov Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "------\n",
      "Llama 2-Chat , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
      "------\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "------\n",
      "variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "We believe that the open release of LLMs, when done safely, will be a net beneﬁt to society. Like all LLMs,\n",
      "Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Loading Documents\n",
    "loader = PyPDFLoader(\"data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Document Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "     # [pages[2].page_content, pages[3].page_content]\n",
    "     [page.page_content for page in pages[:4]]\n",
    ")\n",
    "   \n",
    "\n",
    "# Data Ingestion\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "# db = Chroma.from_documents(texts, embeddings)\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Retrieving Top-5 Results\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"How many parameters does Llama 2 have?\")\n",
    "\n",
    "# print(docs[0].page_content)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a6043",
   "metadata": {},
   "source": [
    "For more links to third-party retrieval components, refer to: https://python.langchain.com/docs/integrations/vectorstores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b47982",
   "metadata": {},
   "source": [
    "### 2.4 Summary\n",
    "1. The document processing part of LangChain is implemented rather roughly and is not recommended for production use.\n",
    "2. The connection to vector databases is essentially an interface encapsulation; you need to choose your own vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83917a1",
   "metadata": {},
   "source": [
    "## 3. Memory Encapsulation: Memory\n",
    "### 3.1 Conversation Context: ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54584b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: How are you?\\nAI: I'm OK\"}\n",
      "{'history': \"Human: How are you?\\nAI: I'm OK\\nHuman: What's up to you today?\\nAI: Everything is going well.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "history.save_context({\"input\": \"How are you?\"}, {\"output\": \"I'm OK\"})\n",
    "\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"What's up to you today?\"}, {\"output\": \"Everything is going well.\"})\n",
    "\n",
    "print(history.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf8fdf",
   "metadata": {},
   "source": [
    "### 3.2 Keep Only One Window's Context: ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ade4b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Second round question\\nAI: Second round answer\\nHuman: Third round question\\nAI: Third round answer'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window = ConversationBufferWindowMemory(k=2)\n",
    "window.save_context({\"input\": \"First round question\"}, {\"output\": \"First round answer\"})\n",
    "window.save_context({\"input\": \"Second round question\"}, {\"output\": \"Second round answer\"})\n",
    "window.save_context({\"input\": \"Third round question\"}, {\"output\": \"Third round answer\"})\n",
    "print(window.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001dc93",
   "metadata": {},
   "source": [
    "### 3.3 Control Context Length by Token Count: ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0591fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Hello\\nAI: Hi, I am your AI assistant.\\nHuman: What can you do?\\nAI: I can do anything.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=ChatOpenAI(),\n",
    "    max_token_limit=40\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello\"}, {\"output\": \"Hi, I am your AI assistant.\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"What can you do?\"}, {\"output\": \"I can do anything.\"}\n",
    ")\n",
    "\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ccd5c",
   "metadata": {},
   "source": [
    "### 3.4 More Types\n",
    "- ConversationSummaryMemory: Summarizes the context\n",
    "  - https://python.langchain.com/docs/modules/memory/types/summary\n",
    "- ConversationSummaryBufferMemory: Saves context within token limits and summarizes older ones\n",
    "  - https://python.langchain.com/docs/modules/memory/types/summary_buffer\n",
    "- VectorStoreRetrieverMemory: Stores memory in a vector database and retrieves the most relevant parts based on user input\n",
    "  - https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c5d48",
   "metadata": {},
   "source": [
    "### 3.5 Summary\n",
    "1. LangChain's memory management mechanism is usable, especially for simple cases like managing by rounds or token count.\n",
    "2. For complex situations, it may not be the optimal implementation; for instance, in retrieving from a vector database, it is advisable to evaluate based on actual conditions and results.\n",
    "3. However, the various maintenance methods for memory can be referenced in actual production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140adc92",
   "metadata": {},
   "source": [
    "## 4. Chain and LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed94432",
   "metadata": {},
   "source": [
    "LangChain Expression Language (LCEL) is a declarative language that allows for easy composition of different call sequences to form a Chain. From its inception, LCEL has been designed to support deploying prototypes into production environments **without code changes**, ranging from the simplest \"prompt + LLM\" chains to the most complex chains (users have successfully run LCEL Chains containing hundreds of steps in production environments).\n",
    "\n",
    "Some highlights of LCEL include:\n",
    "\n",
    "1. **Stream Support**: When building Chains with LCEL, you can achieve optimal first token times (i.e., the time from output start to the generation of the first output). For some Chains, this means you can stream tokens directly from the LLM to the streaming output parser, obtaining parsed, incremental outputs at the same rate as the LLM provider outputs raw tokens.\n",
    "\n",
    "2. **Asynchronous Support**: Any chain built with LCEL can be called through synchronous APIs (e.g., for prototyping in Jupyter notebooks) and asynchronous APIs (e.g., in LangServe servers). This allows the same code to be used for prototyping and production environments, delivering excellent performance and enabling multiple concurrent requests on the same server.\n",
    "\n",
    "3. **Optimized Parallel Execution**: When your LCEL chain has steps that can be executed in parallel (e.g., retrieving documents from multiple retrievers), we automatically execute them with minimal latency, whether in synchronous or asynchronous interfaces.\n",
    "\n",
    "4. **Retry and Fallback**: Configure retries and fallbacks for any part of the LCEL chain. This is an excellent way to make the chain more reliable at scale. We are currently adding support for retry/fallback in streaming so that you can gain increased reliability without adding any latency costs.\n",
    "\n",
    "5. **Access to Intermediate Results**: For more complex chains, accessing the results of intermediate steps before the final output is generated can be very useful. This can be used to inform the end user that something is happening, or simply for debugging the chain. You can stream intermediate results, and they are available on each LangServe server.\n",
    "\n",
    "6. **Input and Output Schemas**: Input and output schemas provide Pydantic and JSONSchema schemas inferred from the structure of each LCEL chain. This can be used for input and output validation and is a component of LangServe.\n",
    "\n",
    "7. **Seamless Integration with LangSmith Tracking**: As chains become more complex, understanding what happens at each step becomes increasingly important. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\n",
    "\n",
    "8. **Seamless Integration with LangServe Deployment**: Any chain created with LCEL can be easily deployed using LangServe.\n",
    "\n",
    "Original text: https://python.langchain.com/docs/expression_language/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc78b57",
   "metadata": {},
   "source": [
    "### 4.1 Pipeline-style Calls: PromptTemplate, LLM, and OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc69d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d64a945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"price_lower\": null,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": null,\n",
      "    \"data_upper\": null,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Output structure\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"Traffic package name\", default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"Lower price limit\", default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"Upper price limit\", default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"Lower data limit\", default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"Upper data limit\", default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"Sort by price or traffic\", default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(\n",
    "        description=\"Sort in ascending or descending order\", default=None)\n",
    "\n",
    "\n",
    "# OutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Semantics)\n",
    "\n",
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Parse the user's input into JSON representation. Output format is as follows:\\n{format_instructions}\\nDo not output fields not mentioned.\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Model\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# LCEL expression\n",
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | model | parser\n",
    ")\n",
    "\n",
    "# Run\n",
    "ret = runnable.invoke(\"What are the large data packages under 100 yuan?\")\n",
    "\n",
    "# print(ret.json())\n",
    "\n",
    "print(\n",
    "  json.dumps(\n",
    "      ret.dict(),\n",
    "      indent = 4,\n",
    "      ensure_ascii=False\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04456d7f",
   "metadata": {},
   "source": [
    "##### stream out format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f2ebffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"price_upper\": 100,\n",
      "  \"sort_by\": \"data\",\n",
      "  \"ordering\": \"descend\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | model |  StrOutputParser()\n",
    ")\n",
    "\n",
    "# stream output \n",
    "\n",
    "for s in runnable.invoke(\"What are the large data packages under 100 yuan?\"):\n",
    "    print(s,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75901c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> <b>Note:</b> In the current documentation, objects generated by LCEL are referred to as runnable or chain, with both terms often used interchangeably. Essentially, it's a custom call flow. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642a0ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> <b>The value of using LCEL is also the core value of LangChain.</b> <br /> The official documentation provides examples from different perspectives: https://python.langchain.com/docs/expression_language/why </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ea7dc",
   "metadata": {},
   "source": [
    "### 4.2 Implementing RAG with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4781dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Loading Documents\n",
    "loader = PyPDFLoader(\"data/llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Document Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    \n",
    "    # [pages[2].page_content, pages[3].page_content]\n",
    "    [page.page_content for page in pages[:4]]\n",
    "\n",
    ")\n",
    "\n",
    "# Data Ingestion\n",
    "OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "# db = Chroma.from_documents(texts, embeddings)\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Retrieving Top-1 Results\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8014eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2 has variants with 7B, 13B, and 70B parameters.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Prompttemplate\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"how many parametes of Llama 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbc0ea",
   "metadata": {},
   "source": [
    "### 4.3 Implementing Function Calling with LCEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7cd69547",
   "metadata": {},
   "outputs": [],
   "source": [
    " llm=ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c1d476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiplying two integers\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Add two integers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Exponentiate the base to the exponent power.\"\n",
    "    return base**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f619aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import JsonOutputToolsParser\n",
    "\n",
    "tools = [multiply, add, exponentiate]\n",
    "# LCEL with branching\n",
    "llm_with_tools = llm.bind_tools(tools) | {\n",
    "    \"functions\": JsonOutputToolsParser(),\n",
    "    \"text\": StrOutputParser()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ae3c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'functions': [{'args': {'first_int': 16, 'second_int': 1024}, 'type': 'multiply'}], 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"What's 16 times 1024?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "359f47e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'functions': [], 'text': 'I am a language model assistant here to help you with any questions or tasks you have. How can I assist you today?'}\n"
     ]
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"Who are you?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b0248",
   "metadata": {},
   "source": [
    "#### Directly selecting tools and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "311450a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "# Mapping from names to functions\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "def call_tool(tool_invocation: dict) -> Union[str, Runnable]:\n",
    "    \"\"\"Function for dynamically constructing the end of the chain based on the model-selected tool.\"\"\"\n",
    "    tool = tool_map[tool_invocation[\"type\"]]\n",
    "    return RunnablePassthrough.assign(\n",
    "        output=itemgetter(\"args\") | tool\n",
    "    )\n",
    "\n",
    "\n",
    "# .map() allows us to apply a function to a list of inputs.\n",
    "call_tool_list = RunnableLambda(call_tool).map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bea64ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': {'base': 1024, 'exponent': 2}, 'type': 'exponentiate', 'output': 1048576}]\n",
      "I'm here and ready to help! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def route(response):\n",
    "    if len(response[\"functions\"]) > 0:\n",
    "        return response[\"functions\"]\n",
    "    else:\n",
    "        return response[\"text\"]\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools) | {\n",
    "    \"functions\": JsonOutputToolsParser() | call_tool_list,\n",
    "    \"text\": StrOutputParser()\n",
    "} | RunnableLambda(route)\n",
    "\n",
    "result = llm_with_tools.invoke(\"What is the square of 1024?\")\n",
    "print(result)\n",
    "\n",
    "result = llm_with_tools.invoke(\"how are you?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa67d25",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> This approach has poor readability. I personally do not recommend using overly complex LCEL structures! </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185cb42",
   "metadata": {},
   "source": [
    "### 4.4 Implementing Factory Pattern with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5814fa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? If you have a specific question or need information on a particular topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# model 1\n",
    "ernie_model = QianfanChatEndpoint(\n",
    "    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),\n",
    "    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')\n",
    ")\n",
    "\n",
    "# model 2\n",
    "gpt_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Use configurable_alternatives to set up field selection models\n",
    "model = gpt_model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"gpt\",\n",
    "    ernie=ernie_model,\n",
    "    gpt4o = gpt_model,\n",
    "    # claude=claude_model\n",
    ")\n",
    "\n",
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\"query\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LCEL\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    |prompt\n",
    "    |model\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n",
    "# Specify the model at runtime: \"gpt\" or \"ernie\" or \"gpt4o\" or claude\n",
    "ret = chain.with_config(configurable={\"llm\": \"gpt\"}).invoke(\"Introduce yourself, and tell me how do you help me?\")\n",
    "\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26a9d9",
   "metadata": {},
   "source": [
    "Further reading: What is the Factory Pattern or the Builder Pattern; Overview of Design Patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1317f3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Thought:</b> What is the significance of LCEL from the perspective of decoupling dependencies between modules?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad4c78",
   "metadata": {},
   "source": [
    "### With LCEL, you can also achieve\n",
    "1. Configuring runtime variables: https://python.langchain.com/docs/expression_language/how_to/configure\n",
    "2. Fallback mechanisms: https://python.langchain.com/docs/expression_language/how_to/fallbacks\n",
    "3. Parallel calls: https://python.langchain.com/docs/expression_language/how_to/map\n",
    "4. Logic branches: https://python.langchain.com/docs/expression_language/how_to/routing\n",
    "5. Calling custom streaming functions: https://python.langchain.com/docs/expression_language/how_to/generators\n",
    "6. Connecting external Memory: https://python.langchain.com/docs/expression_language/how_to/message_history\n",
    "More examples: https://python.langchain.com/docs/expression_language/cookbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39734658",
   "metadata": {},
   "source": [
    "## 5. Intelligent Agent Architecture: Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d2535",
   "metadata": {},
   "source": [
    "### 5.1 Recap: What is an Agent?\n",
    "Treating large language models as reasoning engines. Given a task, the agent automatically generates the necessary steps to complete the task, executes the corresponding actions (such as selecting and calling tools), until the task is completed.\n",
    "\n",
    "<img src=\"data/agent-overview.png\" style=\"margin-left: 0px\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4888044",
   "metadata": {},
   "source": [
    "### 5.2 First, define some tools: Tools\n",
    "- They can be a function or a third-party API\n",
    "- A Chain or the run() method of an Agent can also be treated as a Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "571a8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40d15ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40306f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool, tool\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "255d3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import dateutil.parser as parser\n",
    "from datetime import date\n",
    "\n",
    "# 自定义工具\n",
    "\n",
    "\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    d = parser.parse(date_str)\n",
    "    return calendar.day_name[d.weekday()]\n",
    "\n",
    "\n",
    "tools += [weekday]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c25f5",
   "metadata": {},
   "source": [
    "### 5.3 Types of Agents: ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f978675",
   "metadata": {},
   "source": [
    "<img src=\"data/ReAct.png\" style=\"margin-left: 0px\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74d09def",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:31:11.805070300Z",
     "start_time": "2024-09-22T20:31:08.383961900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\sk_env\\lib\\site-packages\\langchain\\hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "import json\n",
    "\n",
    "# 下载一个现有的 Prompt 模板\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e05e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# # 定义一个 agent: 需要大模型、工具集、和 Prompt 模板\n",
    "# agent = create_react_agent(llm, tools, prompt)\n",
    "# # 定义一个执行器：需要 agent 对象 和 工具集\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# # 执行\n",
    "# agent_executor.invoke({\"input\": \"周杰伦出生那天是星期几\"})\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "from dateutil import parser\n",
    "import calendar\n",
    "\n",
    "# 定义日期解析工具\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    try:\n",
    "        d = parser.parse(date_str)\n",
    "        return calendar.day_name[d.weekday()]\n",
    "    except Exception as e:\n",
    "        return f\"Error parsing date: {e}\"\n",
    "\n",
    "# 搜索 Jay Chou 的出生日期\n",
    "def get_jay_chou_birthday():\n",
    "    return \"1979-01-18\"  # 周杰伦的实际出生日期\n",
    "\n",
    "# 定义 LLM 和工具\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# 定义工具\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=weekday,\n",
    "        name=\"Get Weekday\",\n",
    "        description=\"Converts a date string to the name of the weekday\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 创建 agent 和执行器\n",
    "agent = create_react_agent(llm, tools)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# 执行查找并解析周杰伦的出生日期\n",
    "birthday = get_jay_chou_birthday()  # 获取正确的生日日期\n",
    "result = agent_executor.invoke({\"input\": f\"周杰伦的出生日期是 {birthday}\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc81b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sk_env)",
   "language": "python",
   "name": "sk_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
